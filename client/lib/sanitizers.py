"""
Sanitizers - Fixing LLM Output Quirks and Input Validation
==========================================================

This module contains "middleware" functions that clean and sanitize data
flowing between the agent, LLM, and MCP servers.

Why this exists:
- LLMs sometimes generate invalid or malformed output
- SQL generated by LLMs often has common mistakes
- Input sanitization prevents errors and security issues
- Defensive programming makes the system more robust

Key Concepts:
- **Defensive Programming**: Assume things can go wrong and handle them gracefully
- **Sanitization**: Cleaning untrusted data before using it
- **Middleware**: Code that processes data between components
- **Pattern Matching**: Using regex to detect and fix common mistakes

Learning Points:
- In production systems, you ALWAYS validate and sanitize:
  * User input (prevent SQL injection, XSS)
  * LLM output (fix formatting errors, validate structure)
  * External API responses (handle unexpected formats)
- This layer makes the system resilient to LLM quirks without failing
- Each LLM (GPT, Claude, Llama) has different quirks - document them!

Real-World Example:
    Llama 3.2 sometimes generates SQL like:
        "SELECT * FROM notes WHERE title ILIKE %shopping%"
    But PostgreSQL requires:
        "SELECT * FROM notes WHERE title ILIKE '%shopping%'"

    Instead of failing, we detect and fix this automatically!
"""

import re
import urllib.parse
from typing import Dict, Any


def clean_json_text(text: str) -> str:
    """
    Clean up invalid JSON artifacts from LLM output.

    LLMs sometimes generate JSON with invalid escape sequences or formatting
    errors. This function fixes common issues before JSON parsing.

    Common issues fixed:
    - Invalid escape \\% → %
    - Excessive forward slash escaping \/ → /
    - Escaped quotes at end of string %\\"}} → %"}}
    - Missing colons after keys "parameters" {"sql": ...} → "parameters": {"sql": ...}

    Args:
        text: Potentially malformed JSON string from LLM

    Returns:
        Cleaned JSON string that's more likely to parse correctly

    Learning Point:
        Different LLMs have different quirks. Llama 3 often escapes things
        it shouldn't (like % in SQL patterns). GPT-4 might have different
        issues. This function encodes knowledge about Llama 3's behavior.

    Example:
        >>> clean_json_text('{"sql": "ILIKE \\\\%shopping\\\\%"}')
        '{"sql": "ILIKE %shopping%"}'

        >>> clean_json_text('{"parameters" {"key": "value"}}')
        '{"parameters": {"key": "value"}}'
    """
    if not text:
        return text

    # Fix 1: Replace invalid escape \\% with %
    # Llama 3 sometimes treats % as a special character and escapes it
    text = text.replace(r"\%", "%")

    # Fix 2: Fix excessively escaped forward slashes
    # JSON allows \/ but it's usually unnecessary
    text = text.replace(r"\/", "/")

    # Fix 3: Fix escaped closing quotes at end of strings
    # Pattern: ...%\\"}} should be ...%"}}
    if text.endswith(r'\"}}'):
        text = text[:-4] + '"}}'

    # Fix 4: Fix commonly missing colons or malformed keys
    # Pattern: "parameters" {"sql": ...} → "parameters": {"sql": ...}
    # Pattern: parameters={"sql": ...} → "parameters": {"sql": ...}
    # Pattern: "parameters"={"sql": ...} → "parameters": {"sql": ...}
    text = re.sub(r'\"?(parameters|arguments)\"?\s*[:=]?\s*(\{)', r'"\1": \2', text)

    return text


def fix_sql_args(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Middleware to fix common LLM SQL generation mistakes.

    This function is a "sanitizer" that runs AFTER the LLM generates a tool
    call but BEFORE we execute it. It fixes common mistakes that LLMs make
    when generating SQL queries.

    Fixes applied:
    1. Decode URL-encoded characters (e.g., %27 → ')
    2. Wrap unquoted ILIKE patterns in quotes (%pattern% → '%pattern%')
    3. Clean up LaTeX-style escapes in SQL strings

    Args:
        args: Tool arguments dictionary, potentially containing 'sql' key

    Returns:
        Modified arguments with sanitized SQL

    Learning Points:
    - **Why URL encoding?** Llama 3.2:3b sometimes outputs %27 instead of '
      This might be because it was trained on HTTP logs or URL data
    - **Why ILIKE quotes?** The LLM understands ILIKE needs a pattern, but
      forgets that SQL requires quotes around string literals
    - **Defensive vs Permissive**: We could reject bad SQL, but fixing it
      provides a better user experience for an educational tool

    Security Note:
        This is NOT a substitute for proper SQL injection prevention!
        In production, you should:
        - Use parameterized queries
        - Validate SQL structure (AST parsing)
        - Whitelist allowed tables/operations
        - Limit query complexity (no nested subqueries, etc.)

    Example:
        Input:  {"sql": "SELECT * FROM notes WHERE title ILIKE %shopping%"}
        Output: {"sql": "SELECT * FROM notes WHERE title ILIKE '%shopping%'"}

        Input:  {"sql": "SELECT * WHERE title = %27test%27"}
        Output: {"sql": "SELECT * WHERE title = 'test'"}
    """
    if "sql" not in args or not isinstance(args["sql"], str):
        return args

    sql = args["sql"]

    # Sanitization Step 0: Clean general artifacts that survived JSON parsing
    sql = sql.replace(r"\%", "%")

    # Sanitization Step 1: Decode URL-encoded characters
    # Llama 3.2:3b sometimes outputs %27 instead of ' (single quote)
    # Only decode if it looks suspicious to avoid accidental data corruption
    if "%27" in sql:
        try:
            sql = urllib.parse.unquote(sql)
        except Exception:
            # If decoding fails, leave it as-is
            # Better to fail later with clear error than here silently
            pass

    # Sanitization Step 2: Fix missing quotes in ILIKE clauses
    # Pattern: "ILIKE %something%" → "ILIKE '%something%'"
    # This regex finds ILIKE followed by unquoted %patterns%
    #
    # Explanation of regex: r"ILIKE\s+(%[^']*?%)"
    # - ILIKE\s+ : Match "ILIKE" followed by whitespace
    # - (%[^']*?%) : Capture group matching % ... %
    #   - %        : Literal percent sign
    #   - [^']*?   : Any characters except quotes (non-greedy)
    #   - %        : Closing percent sign
    matches = re.findall(r"ILIKE\s+(%[^']*?%)", sql, flags=re.IGNORECASE)

    for match in matches:
        # Check if already quoted (defensive check)
        if "'" in match:
            continue

        # Wrap the pattern in single quotes
        sql = sql.replace(match, f"'{match}'")

    # Update the arguments
    args["sql"] = sql
    return args


def validate_tool_arguments(tool_name: str, args: Dict[str, Any]) -> bool:
    """
    Validate that tool arguments are reasonable and safe.

    This is a basic validation layer that checks arguments before
    executing tools. In production, this would be much more thorough.

    Args:
        tool_name: Name of the tool being called
        args: Arguments passed to the tool

    Returns:
        True if arguments are valid, raises ValueError otherwise

    Raises:
        ValueError: If arguments are invalid or potentially dangerous

    Learning Point:
        Input validation is the first line of defense against errors
        and security issues. Always validate at system boundaries!

    Example:
        >>> validate_tool_arguments("read_file", {"path": "hello.txt"})
        True

        >>> validate_tool_arguments("read_file", {"path": "../../etc/passwd"})
        ValueError: Potentially dangerous file path
    """
    if tool_name == "read_file":
        path = args.get("path", "")

        # Check for path traversal attempts
        if ".." in path:
            raise ValueError(
                f"Potentially dangerous file path: {path}\n"
                f"Path traversal (.. sequences) not allowed for security."
            )

        # Check for absolute paths (should be relative)
        if path.startswith("/"):
            raise ValueError(
                f"Absolute paths not allowed: {path}\n"
                f"Please use relative paths from the data directory."
            )

    elif tool_name == "query_db":
        sql = args.get("sql", "")

        # Basic sanity check: SQL should not be empty
        if not sql or not sql.strip():
            raise ValueError("SQL query cannot be empty")

        # Warning for potentially dangerous SQL keywords
        # (Not blocked, just logged - this is educational!)
        dangerous_keywords = ["DROP", "TRUNCATE", "DELETE", "ALTER"]
        sql_upper = sql.upper()
        for keyword in dangerous_keywords:
            if keyword in sql_upper:
                # In production, you might block this or require confirmation
                # For education, we allow it but could log a warning
                pass

    return True


def sanitize_output(output: Any, max_length: int = 10000) -> Any:
    """
    Sanitize tool output before sending to LLM.

    Large outputs can cause issues with LLM context windows.
    This function ensures outputs are reasonable in size.

    Args:
        output: Tool output (could be dict, list, string, etc.)
        max_length: Maximum length of string representations

    Returns:
        Sanitized output (truncated if necessary)

    Learning Point:
        LLMs have context limits (e.g., 8K, 32K, 128K tokens).
        Large tool outputs can consume the entire context, leaving
        no room for reasoning. Always consider context management!

    Example:
        >>> large_string = "x" * 50000
        >>> result = sanitize_output(large_string, max_length=1000)
        >>> len(result) <= 1010  # 1000 + truncation message
        True
    """
    import json

    # Convert to string representation for length checking
    if isinstance(output, (dict, list)):
        output_str = json.dumps(output)
    else:
        output_str = str(output)

    # Truncate if too long
    if len(output_str) > max_length:
        truncated = output_str[:max_length]
        return truncated + f"\n\n... (truncated {len(output_str) - max_length} characters)"

    return output
